{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024a1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chathist\n",
    "from chathist import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f58a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 13:16:28,512 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "chathist.config.load_config(config_path='conf/train', config_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538e3177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 13:16:28,516 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-06-01 13:16:28,635 - INFO - Device Selected: cpu\n",
      "2025-06-01 13:16:28,635 - INFO - alpaca style chosen!!\n",
      "2025-06-01 13:16:29,533 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:29,564 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:29,621 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:29,651 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:29,712 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:29,746 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:29,806 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:29,836 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:29,894 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:29,925 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:29,984 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,014 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,072 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,101 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,158 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,188 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,246 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,276 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,333 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,363 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,420 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,478 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,537 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,569 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,628 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,657 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,716 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,747 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,805 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,836 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,894 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:30,924 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:30,981 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,010 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,067 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,096 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,153 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,182 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,238 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,267 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,324 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,353 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,411 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,440 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,497 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,526 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,583 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,612 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,670 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,699 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,756 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,786 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,843 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,873 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:31,929 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:31,958 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,014 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,045 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,103 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,133 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,191 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,221 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,278 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,307 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,364 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,394 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,452 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,482 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,567 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,596 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,653 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,682 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,739 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,768 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,826 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,855 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,912 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:32,942 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:32,998 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,027 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,084 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,113 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,171 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,201 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,259 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,289 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,346 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,376 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,434 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,464 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,521 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,551 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,608 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,638 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,696 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-06-01 13:16:33,726 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-06-01 13:16:33,941 - INFO - Using Lora for final output layer!!\n",
      "2025-06-01 13:16:33,943 - INFO - Loading saved model on path /Users/sameergururajmathad/Documents/CSC - 675/AI Capstone/chathist/models/finetuned/gpt2-xl_indian_history_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f217c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 13:17:05,853 - INFO - Formatting prompt into alpaca style\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archaeologist John Marshall identified the oldest civilization in the Indus region after Charles Masson noticed a large number of seals with seals' impressions in a lunette."
     ]
    }
   ],
   "source": [
    "for token in history_model.generate(\"\"\"Who identified the oldest civilization in the Indus region?\"\"\"):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccbcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
