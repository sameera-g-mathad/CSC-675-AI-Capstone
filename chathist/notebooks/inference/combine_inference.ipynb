{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37106e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chathist\n",
    "from chathist import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8976a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:31:06,916 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "chathist.config.load_config(config_path='conf/train', config_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8bf6851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:31:06,923 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-27 13:31:07,105 - INFO - Device Selected: cuda\n",
      "2025-05-27 13:31:07,106 - INFO - alpaca style chosen!!\n",
      "2025-05-27 13:31:07,674 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:08,152 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:08,265 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:08,341 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:08,453 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:08,531 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:08,646 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:08,724 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:08,839 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:08,914 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:09,027 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:09,101 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:09,213 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:09,287 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:09,398 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:09,472 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:09,585 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:09,658 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:09,770 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:09,844 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:09,956 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:10,028 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:10,140 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:10,211 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:10,323 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:10,395 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:10,508 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:10,580 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:10,693 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:10,766 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:10,881 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:10,954 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:11,067 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:11,142 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:11,255 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:11,328 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:11,441 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:11,514 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:11,628 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:11,703 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:11,817 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:11,891 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:12,006 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:12,079 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:12,193 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:12,266 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:12,380 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:12,453 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:12,568 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:12,641 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:12,754 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:12,827 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:12,941 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:13,013 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:13,128 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:13,200 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:13,313 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:13,386 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:13,499 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:13,572 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:13,690 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:13,764 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:13,877 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:13,950 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:14,063 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:14,134 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:14,247 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:14,321 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:14,434 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:14,506 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:14,620 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:14,695 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:14,808 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:14,881 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:14,994 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:15,068 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:15,182 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:15,256 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:15,370 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:15,444 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:15,556 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:15,629 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:15,743 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:15,818 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:15,933 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:16,006 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:16,120 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:16,194 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:16,308 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:16,381 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:16,495 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:16,567 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:16,682 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:16,753 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:16,866 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:16,939 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:17,364 - INFO - Using Lora for final output layer!!\n",
      "2025-05-27 13:31:17,370 - INFO - Loading saved model on path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-xl_indian_history_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70b5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:31:25,944 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "chathist.config.load_config(config_path=\"conf/train\", config_name=\"chat_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8454ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:31:25,949 - INFO - Device Selected: cuda\n",
      "2025-05-27 13:31:25,950 - INFO - phi3 style chosen!!\n",
      "2025-05-27 13:31:26,287 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,323 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,378 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,413 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,472 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,507 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,564 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,599 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,656 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,691 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,748 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,782 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,838 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,873 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:26,930 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:26,964 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,022 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,056 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,112 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,146 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,203 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,237 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,293 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,326 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,382 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,416 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,473 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,507 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,564 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,597 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,654 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,685 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,742 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,777 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,833 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,866 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:27,923 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:27,957 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:28,013 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:28,048 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:28,106 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:28,140 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:28,196 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:28,228 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:28,285 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:28,323 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:28,380 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:31:28,415 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:31:28,700 - INFO - Using Lora for final output layer!!\n",
      "2025-05-27 13:31:28,705 - INFO - Loading saved model on path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-medium_chat_title_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da032ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:31:31,798 - INFO - Formatting prompt into alpaca style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Kalidasa's compositions showcased cultural nuances such as love, seasons, and nature, providing a nuanced portrayal of the societal ethos and aesthetic sensibilities during the Gupta era.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.generate(\"How did Kalidasa's works reflect Gupta society's cultural aspects?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd84ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:35:51,846 - INFO - Formatting prompt into phi3 style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Gupta Society's Cultural Insights\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_model.generate(\"How did Kalidasa's works reflect Gupta society's cultural aspects? Kalidasa's compositions showcased cultural nuances such as love, seasons, and nature, providing a nuanced portrayal of the societal ethos and aesthetic sensibilities during the Gupta era.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
