{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37106e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chathist\n",
    "from chathist import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8976a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:52:38,835 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "chathist.config.load_config(config_path='conf/train', config_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8bf6851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:52:38,844 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-27 13:52:39,009 - INFO - Device Selected: cuda\n",
      "2025-05-27 13:52:39,010 - INFO - alpaca style chosen!!\n",
      "2025-05-27 13:52:39,567 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:39,951 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:40,062 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:40,140 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:40,253 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:40,329 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:40,442 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:40,516 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:40,629 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:40,701 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:40,812 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:40,885 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:40,996 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:41,068 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:41,179 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:41,259 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:41,373 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:41,444 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:41,554 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:41,627 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:41,739 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:41,811 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:41,924 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:41,996 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:42,107 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:42,179 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:42,291 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:42,364 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:42,477 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:42,549 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:42,662 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:42,734 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:42,848 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:42,921 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:43,035 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:43,108 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:43,221 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:43,295 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:43,407 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:43,480 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:43,593 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:43,665 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:43,777 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:43,849 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:43,962 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:44,035 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:44,148 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:44,220 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:44,334 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:44,406 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:44,518 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:44,591 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:44,703 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:44,776 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:44,889 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:44,962 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:45,074 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:45,147 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:45,260 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:45,332 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:45,445 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:45,515 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:45,627 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:45,700 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:45,816 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:45,893 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:46,006 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:46,079 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:46,192 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:46,265 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:46,377 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:46,451 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:46,563 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:46,636 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:46,748 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:46,820 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:46,932 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:47,003 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:47,115 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:47,188 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:47,300 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:47,371 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:47,483 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:47,555 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:47,667 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:47,739 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:47,850 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:47,922 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:48,034 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:48,105 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:48,218 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:48,290 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:48,402 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:48,474 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:48,586 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:48,656 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:49,073 - INFO - Using Lora for final output layer!!\n",
      "2025-05-27 13:52:49,087 - INFO - Loading saved model on path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-xl_indian_history_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70b5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:52:54,712 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "chathist.config.load_config(config_path=\"conf/train\", config_name=\"chat_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8454ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:52:54,719 - INFO - Device Selected: cuda\n",
      "2025-05-27 13:52:54,720 - INFO - phi3 style chosen!!\n",
      "2025-05-27 13:52:55,055 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,090 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,143 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,178 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,234 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,270 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,326 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,361 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,417 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,453 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,509 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,544 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,600 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,634 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,690 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,724 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,781 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,815 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,871 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,905 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:55,961 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:55,994 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,050 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,084 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,140 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,175 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,230 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,263 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,319 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,353 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,409 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,442 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,498 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,532 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,588 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,622 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,678 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,711 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,767 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,800 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,856 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,890 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:56,946 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:56,980 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:57,036 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:57,070 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:57,126 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-27 13:52:57,161 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-27 13:52:57,443 - INFO - Using Lora for final output layer!!\n",
      "2025-05-27 13:52:57,450 - INFO - Loading saved model on path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-medium_chat_title_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc6665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "## Input:\\nWhat literary works provide information on Mauryan polity? Which texts contain stories of previous Buddha births?\\n## Response:\\nArthasastra by Kautilya offers rich material for studying Indian economy and the political landscape of the Mauryan period. The Jatakas, containing stories of the previous births of Buddha, provide insights into social and economic conditions between the fifth and second centuries BC.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da032ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:55:53,844 - INFO - Formatting prompt into alpaca style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Tripitakas are three baskets of Buddhist texts: Suttapittaka, Vinayapitaka, and Abhidhammapitaka. They contain teachings, rules for monks, and philosophical discussions.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.generate(f\"What are the Tripitakas in Buddhism?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd84ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:53:04,113 - INFO - Formatting prompt into phi3 style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Gupta Society's Cultural Insights\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_model.generate(\"How did Kalidasa's works reflect Gupta society's cultural aspects? Kalidasa's compositions showcased cultural nuances such as love, seasons, and nature, providing a nuanced portrayal of the societal ethos and aesthetic sensibilities during the Gupta era.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
