{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de45921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameergururajmathad/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chathist import Model\n",
    "# import chathist\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85922a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 19:34:19,700 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-10 19:34:24,563 - INFO - Repo: openai-community/gpt2-xl.\n",
      "2025-05-10 19:34:24,564 - INFO - openai-community/gpt2-xl exists in /Users/sameergururajmathad/Documents/CSC - 675/AI Capstone/chathist/models/pretrained/gpt2-xl. Skipping Download...\n",
      "2025-05-10 19:34:25,371 - INFO - Loading weights into model.\n",
      "2025-05-10 19:34:25,572 - INFO - Using Lora for final output layer!!\n",
      "2025-05-10 19:34:25,573 - INFO - Layer: 1\n",
      "2025-05-10 19:34:25,574 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:25,574 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:25,575 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:25,628 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:25,628 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:25,657 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:25,658 - INFO - Layer: 2\n",
      "2025-05-10 19:34:25,659 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:25,659 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:25,659 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:25,712 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:25,713 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:25,742 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:25,743 - INFO - Layer: 3\n",
      "2025-05-10 19:34:25,743 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:25,743 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:25,744 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:25,881 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:25,882 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:25,912 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:25,913 - INFO - Layer: 4\n",
      "2025-05-10 19:34:25,913 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:25,913 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:25,913 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:26,061 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:26,062 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:26,131 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:26,132 - INFO - Layer: 5\n",
      "2025-05-10 19:34:26,132 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:26,133 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:26,133 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:26,271 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:26,272 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:26,335 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:26,336 - INFO - Layer: 6\n",
      "2025-05-10 19:34:26,336 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:26,336 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:26,336 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:26,583 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:26,584 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:26,627 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:26,628 - INFO - Layer: 7\n",
      "2025-05-10 19:34:26,628 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:26,628 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:26,629 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:26,879 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:26,880 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:26,912 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:26,913 - INFO - Layer: 8\n",
      "2025-05-10 19:34:26,913 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:26,914 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:26,914 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:27,080 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:27,081 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:27,110 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:27,111 - INFO - Layer: 9\n",
      "2025-05-10 19:34:27,111 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:27,111 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:27,112 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:27,242 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:27,243 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:27,275 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:27,276 - INFO - Layer: 10\n",
      "2025-05-10 19:34:27,276 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:27,277 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:27,277 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:27,417 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:27,418 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:27,471 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:27,472 - INFO - Layer: 11\n",
      "2025-05-10 19:34:27,473 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:27,473 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:27,473 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:27,655 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:27,656 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:27,686 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:27,687 - INFO - Layer: 12\n",
      "2025-05-10 19:34:27,687 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:27,687 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:27,687 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:27,808 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:27,808 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:27,839 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:27,840 - INFO - Layer: 13\n",
      "2025-05-10 19:34:27,840 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:27,840 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:27,840 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:28,012 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:28,013 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:28,045 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:28,046 - INFO - Layer: 14\n",
      "2025-05-10 19:34:28,046 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:28,046 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:28,047 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:28,288 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:28,290 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:28,390 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:28,391 - INFO - Layer: 15\n",
      "2025-05-10 19:34:28,391 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:28,391 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:28,391 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:28,628 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:28,629 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:28,663 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:28,664 - INFO - Layer: 16\n",
      "2025-05-10 19:34:28,664 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:28,664 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:28,665 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:28,975 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:28,976 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:29,083 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:29,084 - INFO - Layer: 17\n",
      "2025-05-10 19:34:29,084 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:29,084 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:29,085 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:29,261 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:29,262 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:29,292 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:29,293 - INFO - Layer: 18\n",
      "2025-05-10 19:34:29,293 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:29,294 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:29,294 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:29,538 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:29,539 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:29,570 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:29,571 - INFO - Layer: 19\n",
      "2025-05-10 19:34:29,571 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:29,571 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:29,572 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:29,727 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:29,728 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:29,763 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:29,764 - INFO - Layer: 20\n",
      "2025-05-10 19:34:29,765 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:29,767 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:29,769 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:30,081 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:30,082 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:30,238 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:30,239 - INFO - Layer: 21\n",
      "2025-05-10 19:34:30,239 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:30,240 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:30,240 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:30,390 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:30,391 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:30,430 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:30,431 - INFO - Layer: 22\n",
      "2025-05-10 19:34:30,431 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:30,432 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:30,432 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:30,589 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:30,590 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:30,637 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:30,638 - INFO - Layer: 23\n",
      "2025-05-10 19:34:30,638 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:30,638 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:30,639 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:30,831 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:30,832 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:30,869 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:30,870 - INFO - Layer: 24\n",
      "2025-05-10 19:34:30,870 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:30,871 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:30,872 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:31,099 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:31,100 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:31,140 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:31,141 - INFO - Layer: 25\n",
      "2025-05-10 19:34:31,141 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:31,142 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:31,142 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:31,296 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:31,297 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:31,333 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:31,334 - INFO - Layer: 26\n",
      "2025-05-10 19:34:31,335 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:31,337 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:31,337 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:31,507 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:31,508 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:31,548 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:31,549 - INFO - Layer: 27\n",
      "2025-05-10 19:34:31,549 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:31,550 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:31,550 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:31,711 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:31,711 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:31,763 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:31,764 - INFO - Layer: 28\n",
      "2025-05-10 19:34:31,764 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:31,765 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:31,765 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:31,932 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:31,933 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:31,964 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:31,965 - INFO - Layer: 29\n",
      "2025-05-10 19:34:31,965 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:31,965 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:31,966 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:32,105 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:32,105 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:32,144 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:32,145 - INFO - Layer: 30\n",
      "2025-05-10 19:34:32,145 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:32,145 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:32,146 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:32,296 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:32,297 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:32,336 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:32,337 - INFO - Layer: 31\n",
      "2025-05-10 19:34:32,337 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:32,337 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:32,338 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:32,510 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:32,511 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:32,548 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:32,549 - INFO - Layer: 32\n",
      "2025-05-10 19:34:32,550 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:32,550 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:32,551 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:32,713 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:32,714 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:32,760 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:32,761 - INFO - Layer: 33\n",
      "2025-05-10 19:34:32,761 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:32,761 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:32,762 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:32,935 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:32,936 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:32,983 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:32,984 - INFO - Layer: 34\n",
      "2025-05-10 19:34:32,984 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:32,985 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:32,985 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:33,159 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:33,159 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:33,192 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:33,193 - INFO - Layer: 35\n",
      "2025-05-10 19:34:33,194 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:33,194 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:33,194 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:33,407 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:33,408 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:33,465 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:33,466 - INFO - Layer: 36\n",
      "2025-05-10 19:34:33,466 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:33,467 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:33,467 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:33,626 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:33,627 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:33,679 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:33,680 - INFO - Layer: 37\n",
      "2025-05-10 19:34:33,680 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:33,680 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:33,681 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:33,919 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:33,920 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:33,950 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:33,951 - INFO - Layer: 38\n",
      "2025-05-10 19:34:33,952 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:33,952 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:33,952 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:34,109 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:34,110 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:34,157 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:34,158 - INFO - Layer: 39\n",
      "2025-05-10 19:34:34,158 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:34,159 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:34,159 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:34,314 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:34,315 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:34,351 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:34,352 - INFO - Layer: 40\n",
      "2025-05-10 19:34:34,352 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:34,353 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:34,353 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:34,515 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:34,516 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:34,563 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:34,564 - INFO - Layer: 41\n",
      "2025-05-10 19:34:34,564 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:34,565 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:34,565 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:34,742 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:34,743 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:34,780 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:34,781 - INFO - Layer: 42\n",
      "2025-05-10 19:34:34,781 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:34,782 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:34,782 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:34,940 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:34,941 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:34,982 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:34,983 - INFO - Layer: 43\n",
      "2025-05-10 19:34:34,984 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:34,984 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:34,984 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:35,156 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:35,157 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:35,196 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:35,197 - INFO - Layer: 44\n",
      "2025-05-10 19:34:35,197 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:35,197 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:35,198 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:35,360 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:35,361 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:35,432 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:35,433 - INFO - Layer: 45\n",
      "2025-05-10 19:34:35,433 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:35,434 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:35,434 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:35,601 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:35,602 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:35,661 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:35,662 - INFO - Layer: 46\n",
      "2025-05-10 19:34:35,662 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:35,663 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:35,663 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:35,858 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:35,859 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:35,919 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:35,920 - INFO - Layer: 47\n",
      "2025-05-10 19:34:35,920 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:35,921 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:35,921 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:36,122 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:36,123 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:36,166 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-10 19:34:36,167 - INFO - Layer: 48\n",
      "2025-05-10 19:34:36,167 - INFO - loading norm1 weights and bias\n",
      "2025-05-10 19:34:36,167 - INFO - loading norm2 weights and bias\n",
      "2025-05-10 19:34:36,168 - INFO - loading ff weights and bias\n",
      "2025-05-10 19:34:36,337 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-10 19:34:36,338 - INFO - loading attention weights and bias\n",
      "2025-05-10 19:34:36,374 - INFO - Using LoRA weights for multi head layers!!\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6335b61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652364970"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model.get_model_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8832634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is AI?\\n\\nAI is the field of artificial intelligence, which is the study of intelligent systems. It is the study of the design and implementation of intelligent systems'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebccb54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
