{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d09df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chathist import InstructionDataLoader,InstructionDataset, InstructionStyle, Model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a17c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.read_csv(\"hf://datasets/BashitAli/Indian_history/datasetfile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3681d48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of studying history?</td>\n",
       "      <td>History helps us understand how early humans a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a historian evaluate events?</td>\n",
       "      <td>Historians assess various situations over a lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the historian's role in using myths?</td>\n",
       "      <td>Historians aim to verify facts from myths root...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How were ancient Indian historical events reco...</td>\n",
       "      <td>Ancient Indian history was recorded through a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who identified the oldest civilization in the ...</td>\n",
       "      <td>Archaeologist John Marshall identified the old...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0           What is the purpose of studying history?   \n",
       "1              How does a historian evaluate events?   \n",
       "2       What is the historian's role in using myths?   \n",
       "3  How were ancient Indian historical events reco...   \n",
       "4  Who identified the oldest civilization in the ...   \n",
       "\n",
       "                                            response  \n",
       "0  History helps us understand how early humans a...  \n",
       "1  Historians assess various situations over a lo...  \n",
       "2  Historians aim to verify facts from myths root...  \n",
       "3  Ancient Indian history was recorded through a ...  \n",
       "4  Archaeologist John Marshall identified the old...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e82643f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14908, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090ba88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instruction', 'response'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eeec914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:27:05,478 - INFO - alpaca style chosen!!\n"
     ]
    }
   ],
   "source": [
    "style = InstructionStyle.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09489b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = style.convert_train(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecac74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:, :]\n",
    "# val_df = df.iloc[train_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e31ec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14903</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14904</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14905</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14906</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14907</th>\n",
       "      <td>### You are an assistant with deep expertise i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14908 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                instruct\n",
       "0      ### You are an assistant with deep expertise i...\n",
       "1      ### You are an assistant with deep expertise i...\n",
       "2      ### You are an assistant with deep expertise i...\n",
       "3      ### You are an assistant with deep expertise i...\n",
       "4      ### You are an assistant with deep expertise i...\n",
       "...                                                  ...\n",
       "14903  ### You are an assistant with deep expertise i...\n",
       "14904  ### You are an assistant with deep expertise i...\n",
       "14905  ### You are an assistant with deep expertise i...\n",
       "14906  ### You are an assistant with deep expertise i...\n",
       "14907  ### You are an assistant with deep expertise i...\n",
       "\n",
       "[14908 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4e1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### You are an assistant with deep expertise in Indian history. You will answer any questions related to Indian history, providing accurate and insightful information.\\n## Input:\n",
      "What is the purpose of studying history?\n",
      "## Response:\n",
      "History helps us understand how early humans adapted to their environment and developed civilizations. It involves analyzing society, economy, and culture over time to understand their impact.\n"
     ]
    }
   ],
   "source": [
    "print(train_df['instruct'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27753c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc2214a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val_df['instruct'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68458b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:27:05,620 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n"
     ]
    }
   ],
   "source": [
    "train_dataset = InstructionDataset(train_df['instruct'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5bf387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = InstructionDataset(val_df['instruct'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b87297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = InstructionDataLoader().load(dataset = train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b952b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loader = InstructionDataLoader().load(dataset = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db267fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bb37dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d357e590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21017,   921,   389,   281,  8796,   351,  2769, 13572,   287,  3942,\n",
       "         2106,    13,   921,   481,  3280,   597,  2683,  3519,   284,  3942,\n",
       "         2106,    11,  4955,  7187,   290, 41696,  1321,    13,    59,    77,\n",
       "         2235, 23412,    25,   198,  1867,  2370,  6971,   262,  2113,  1324,\n",
       "          504,     6,  4901,   287,   257,  4257, 26462,    11,   290,   703,\n",
       "          318,   428, 26462, 18904,    30,   198,  2235, 18261,    25,   198,\n",
       "          464,  2113,  1324,   504,     6,  4901,   287,   257,  4257, 26462,\n",
       "          318,  4855,   416, 29718, 27561,   257, 26462,   351,   257, 45019,\n",
       "           12, 25311,   276,  1182,    12, 49380,  5586,   287,   257, 27423,\n",
       "          291, 24521,    11, 11191,   416,  4695,    13,   770, 26462,   318,\n",
       "         5174,   355,   350,  1077,   929,  7246,   393,   705,  2964,  1462,\n",
       "           12,  2484, 12151,  2637, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256, 50256], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61028cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   464,\n",
       "         2113,  1324,   504,     6,  4901,   287,   257,  4257, 26462,   318,\n",
       "         4855,   416, 29718, 27561,   257, 26462,   351,   257, 45019,    12,\n",
       "        25311,   276,  1182,    12, 49380,  5586,   287,   257, 27423,   291,\n",
       "        24521,    11, 11191,   416,  4695,    13,   770, 26462,   318,  5174,\n",
       "          355,   350,  1077,   929,  7246,   393,   705,  2964,  1462,    12,\n",
       "         2484, 12151,  2637, 50256,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5218f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:27:07,204 - INFO - alpaca style chosen!!\n",
      "2025-05-15 10:27:07,777 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:08,116 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:08,229 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:08,308 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:08,421 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:08,495 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:08,608 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:08,681 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:08,796 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:08,871 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:08,984 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:09,058 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:09,179 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:09,252 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:09,364 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:09,433 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:09,545 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:09,618 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:09,732 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:09,804 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:09,918 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:09,990 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:10,104 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:10,178 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:10,291 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:10,361 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:10,473 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:10,547 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:10,661 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:10,731 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:10,846 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:10,916 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:11,028 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:11,098 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:11,209 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:11,280 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:11,399 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:11,473 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:11,587 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:11,659 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:11,770 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:11,840 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:11,950 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:12,026 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:12,138 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:12,212 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:12,325 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:12,399 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:12,512 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:12,584 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:12,696 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:12,766 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:12,878 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:12,948 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:13,059 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:13,129 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:13,240 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:13,310 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:13,421 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:13,492 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:13,604 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:13,674 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:13,786 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:13,856 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:13,969 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:14,042 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:14,157 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:14,227 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:14,339 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:14,409 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:14,520 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:14,592 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:14,708 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:14,782 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:14,896 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:14,966 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:15,078 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:15,147 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:15,259 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:15,330 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:15,441 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:15,512 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:15,624 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:15,694 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:15,806 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:15,878 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:15,992 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:16,067 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:16,180 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:16,250 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:16,361 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:16,431 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:16,542 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:16,612 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:16,730 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-15 10:27:16,800 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-15 10:27:17,223 - INFO - Using Lora for final output layer!!\n",
      "2025-05-15 10:27:17,238 - INFO - Repo: openai-community/gpt2-xl.\n",
      "2025-05-15 10:27:17,241 - INFO - openai-community/gpt2-xl exists in /home/smathad/ai_capstone/chathist/models/pretrained/gpt2-xl. Skipping Download...\n",
      "2025-05-15 10:27:17,709 - INFO - Loading weights into model.\n",
      "2025-05-15 10:27:17,711 - WARNING - Using loRA will freeze the whole model except for loRA weights.\n",
      "2025-05-15 10:27:17,808 - INFO - Layer: 1\n",
      "2025-05-15 10:27:17,809 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,810 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,810 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:17,823 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:17,836 - INFO - Layer: 2\n",
      "2025-05-15 10:27:17,836 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,837 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,838 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:17,856 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:17,866 - INFO - Layer: 3\n",
      "2025-05-15 10:27:17,867 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,867 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,868 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:17,885 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:17,896 - INFO - Layer: 4\n",
      "2025-05-15 10:27:17,896 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,897 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,897 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:17,916 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:17,927 - INFO - Layer: 5\n",
      "2025-05-15 10:27:17,927 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,928 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,929 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:17,947 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:17,958 - INFO - Layer: 6\n",
      "2025-05-15 10:27:17,958 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,959 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,960 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:17,977 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:17,988 - INFO - Layer: 7\n",
      "2025-05-15 10:27:17,989 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:17,989 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:17,990 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,007 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,019 - INFO - Layer: 8\n",
      "2025-05-15 10:27:18,020 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,021 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,022 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,041 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,059 - INFO - Layer: 9\n",
      "2025-05-15 10:27:18,060 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,061 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,063 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,083 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,109 - INFO - Layer: 10\n",
      "2025-05-15 10:27:18,111 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,112 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,113 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,127 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,135 - INFO - Layer: 11\n",
      "2025-05-15 10:27:18,135 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,137 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,137 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,149 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,157 - INFO - Layer: 12\n",
      "2025-05-15 10:27:18,158 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,158 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,159 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,171 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,182 - INFO - Layer: 13\n",
      "2025-05-15 10:27:18,183 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,183 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,184 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,200 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,209 - INFO - Layer: 14\n",
      "2025-05-15 10:27:18,211 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,212 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,212 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,225 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,233 - INFO - Layer: 15\n",
      "2025-05-15 10:27:18,234 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,234 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,236 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,249 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,258 - INFO - Layer: 16\n",
      "2025-05-15 10:27:18,259 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,260 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,262 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,274 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,282 - INFO - Layer: 17\n",
      "2025-05-15 10:27:18,283 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,284 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,285 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,299 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,309 - INFO - Layer: 18\n",
      "2025-05-15 10:27:18,310 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,311 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,311 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,329 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,339 - INFO - Layer: 19\n",
      "2025-05-15 10:27:18,340 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,341 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,343 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,360 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,372 - INFO - Layer: 20\n",
      "2025-05-15 10:27:18,373 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,374 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,376 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,394 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,404 - INFO - Layer: 21\n",
      "2025-05-15 10:27:18,405 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,407 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,407 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,429 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,440 - INFO - Layer: 22\n",
      "2025-05-15 10:27:18,441 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,442 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,444 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,462 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,490 - INFO - Layer: 23\n",
      "2025-05-15 10:27:18,491 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,492 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,492 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,514 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,545 - INFO - Layer: 24\n",
      "2025-05-15 10:27:18,546 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,546 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,547 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,569 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,596 - INFO - Layer: 25\n",
      "2025-05-15 10:27:18,597 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,597 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,598 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,613 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,632 - INFO - Layer: 26\n",
      "2025-05-15 10:27:18,632 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,633 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,634 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,653 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,662 - INFO - Layer: 27\n",
      "2025-05-15 10:27:18,663 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,664 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,664 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,682 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,700 - INFO - Layer: 28\n",
      "2025-05-15 10:27:18,700 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,701 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,702 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,722 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,734 - INFO - Layer: 29\n",
      "2025-05-15 10:27:18,735 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,736 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,737 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,755 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,767 - INFO - Layer: 30\n",
      "2025-05-15 10:27:18,768 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,769 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,770 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,789 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,800 - INFO - Layer: 31\n",
      "2025-05-15 10:27:18,801 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,801 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,802 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,820 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,832 - INFO - Layer: 32\n",
      "2025-05-15 10:27:18,833 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,835 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,837 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,855 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,865 - INFO - Layer: 33\n",
      "2025-05-15 10:27:18,866 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,867 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,868 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,888 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,899 - INFO - Layer: 34\n",
      "2025-05-15 10:27:18,901 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,902 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,903 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,921 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,932 - INFO - Layer: 35\n",
      "2025-05-15 10:27:18,933 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,938 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,939 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,961 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,971 - INFO - Layer: 36\n",
      "2025-05-15 10:27:18,971 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,972 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,972 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:18,983 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:18,991 - INFO - Layer: 37\n",
      "2025-05-15 10:27:18,992 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:18,992 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:18,993 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,007 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,014 - INFO - Layer: 38\n",
      "2025-05-15 10:27:19,015 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,016 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,016 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,029 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,037 - INFO - Layer: 39\n",
      "2025-05-15 10:27:19,038 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,038 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,039 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,054 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,067 - INFO - Layer: 40\n",
      "2025-05-15 10:27:19,068 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,069 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,069 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,088 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,099 - INFO - Layer: 41\n",
      "2025-05-15 10:27:19,100 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,100 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,101 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,118 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,128 - INFO - Layer: 42\n",
      "2025-05-15 10:27:19,129 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,129 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,130 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,147 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,159 - INFO - Layer: 43\n",
      "2025-05-15 10:27:19,160 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,160 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,161 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,177 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,188 - INFO - Layer: 44\n",
      "2025-05-15 10:27:19,189 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,189 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,190 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,219 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,235 - INFO - Layer: 45\n",
      "2025-05-15 10:27:19,235 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,236 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,236 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,257 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,276 - INFO - Layer: 46\n",
      "2025-05-15 10:27:19,276 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,276 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,279 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,298 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,330 - INFO - Layer: 47\n",
      "2025-05-15 10:27:19,332 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,333 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,333 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,348 - INFO - loading attention weights and bias\n",
      "2025-05-15 10:27:19,356 - INFO - Layer: 48\n",
      "2025-05-15 10:27:19,356 - INFO - loading norm1 weights and bias\n",
      "2025-05-15 10:27:19,356 - INFO - loading norm2 weights and bias\n",
      "2025-05-15 10:27:19,357 - INFO - loading ff weights and bias\n",
      "2025-05-15 10:27:19,369 - INFO - loading attention weights and bias\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99f84052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:27:20,824 - INFO - Total model parameters: 1660970512\n",
      "2025-05-15 10:27:20,825 - INFO - Total trainable parameters: 22948112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_name</th>\n",
       "      <th>requires_grad</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tok_emb.weight</td>\n",
       "      <td>False</td>\n",
       "      <td>80411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos_emb.weight</td>\n",
       "      <td>False</td>\n",
       "      <td>1638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformers.layer_0.norm1.scale</td>\n",
       "      <td>False</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformers.layer_0.norm1.shift</td>\n",
       "      <td>False</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformers.layer_0.norm2.scale</td>\n",
       "      <td>False</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>final_norm.scale</td>\n",
       "      <td>False</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>final_norm.shift</td>\n",
       "      <td>False</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>final_layer.linear_layer.weight</td>\n",
       "      <td>False</td>\n",
       "      <td>80411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>final_layer.lora.a_matrix</td>\n",
       "      <td>True</td>\n",
       "      <td>25600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>final_layer.lora.b_matrix</td>\n",
       "      <td>True</td>\n",
       "      <td>804112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1351 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            layer_name  requires_grad    params\n",
       "0                       tok_emb.weight          False  80411200\n",
       "1                       pos_emb.weight          False   1638400\n",
       "2     transformers.layer_0.norm1.scale          False      1600\n",
       "3     transformers.layer_0.norm1.shift          False      1600\n",
       "4     transformers.layer_0.norm2.scale          False      1600\n",
       "...                                ...            ...       ...\n",
       "1346                  final_norm.scale          False      1600\n",
       "1347                  final_norm.shift          False      1600\n",
       "1348   final_layer.linear_layer.weight          False  80411200\n",
       "1349         final_layer.lora.a_matrix           True     25600\n",
       "1350         final_layer.lora.b_matrix           True    804112\n",
       "\n",
       "[1351 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cb7b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 10:27:20,838 - INFO - Epoch 1\n",
      "100%|\u001b[32m██████████\u001b[0m| 931/931 [16:18<00:00,  1.05s/it]\n",
      "2025-05-15 10:43:39,841 - INFO - Train Loss: 1.7872643729413746\n",
      "2025-05-15 10:43:39,845 - INFO - Epoch 2\n",
      "100%|\u001b[32m██████████\u001b[0m| 931/931 [16:17<00:00,  1.05s/it]\n",
      "2025-05-15 10:59:57,761 - INFO - Train Loss: 1.179936866306977\n",
      "2025-05-15 10:59:57,764 - INFO - Epoch 3\n",
      "100%|\u001b[32m██████████\u001b[0m| 931/931 [16:17<00:00,  1.05s/it]\n",
      "2025-05-15 11:16:15,240 - INFO - Train Loss: 0.7926215434689015\n",
      "2025-05-15 11:16:15,244 - INFO - Epoch 4\n",
      "100%|\u001b[32m██████████\u001b[0m| 931/931 [16:17<00:00,  1.05s/it]\n",
      "2025-05-15 11:32:32,427 - INFO - Train Loss: 0.5591055884653208\n",
      "2025-05-15 11:32:32,431 - INFO - Epoch 5\n",
      "100%|\u001b[32m██████████\u001b[0m| 931/931 [16:17<00:00,  1.05s/it]\n",
      "2025-05-15 11:48:50,043 - INFO - Train Loss: 0.4118074157408306\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = model.train(train_loader = train_loader, val_loader = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0004e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### You are an assistant with deep expertise in Indian history. You will answer any questions related to Indian history, providing accurate and insightful information.\\n## Input:\n",
      "How do geological studies contribute to understanding history?\n",
      "## Response:\n",
      "Geological studies help trace the history of soil, rocks, and the environment where prehistoric humans lived, offering insights into the interaction between nature and human development.\n"
     ]
    }
   ],
   "source": [
    "print(train_df['instruct'].values[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b66864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 11:49:52,254 - INFO - Formatting prompt into alpaca style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Bhakti movement.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate('''What movement facilitated the growth and development of regional languages?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049320c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
