{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de45921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chathist import Model\n",
    "# import chathist\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85922a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 12:32:22,445 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-11 12:32:27,841 - INFO - Repo: openai-community/gpt2-xl.\n",
      "2025-05-11 12:32:27,842 - INFO - openai-community/gpt2-xl exists in /Users/sameergururajmathad/Documents/CSC - 675/AI Capstone/chathist/models/pretrained/gpt2-xl. Skipping Download...\n",
      "2025-05-11 12:32:28,282 - INFO - Loading weights into model.\n",
      "2025-05-11 12:32:29,276 - INFO - Using Lora for final output layer!!\n",
      "2025-05-11 12:32:29,278 - INFO - Layer: 1\n",
      "2025-05-11 12:32:29,278 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:29,278 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:29,279 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:29,477 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:29,478 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:29,573 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:29,575 - INFO - Layer: 2\n",
      "2025-05-11 12:32:29,575 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:29,575 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:29,576 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:29,696 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:29,698 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:29,767 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:29,769 - INFO - Layer: 3\n",
      "2025-05-11 12:32:29,769 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:29,769 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:29,770 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:29,914 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:29,915 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:29,957 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:29,959 - INFO - Layer: 4\n",
      "2025-05-11 12:32:29,959 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:29,959 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:29,960 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:30,153 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:30,154 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:30,197 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:30,198 - INFO - Layer: 5\n",
      "2025-05-11 12:32:30,198 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:30,198 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:30,199 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:30,380 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:30,383 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:30,459 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:30,460 - INFO - Layer: 6\n",
      "2025-05-11 12:32:30,460 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:30,461 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:30,461 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:30,657 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:30,658 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:30,753 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:30,755 - INFO - Layer: 7\n",
      "2025-05-11 12:32:30,755 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:30,755 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:30,756 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:30,913 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:30,914 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:30,957 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:30,958 - INFO - Layer: 8\n",
      "2025-05-11 12:32:30,958 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:30,959 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:30,959 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:31,154 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:31,156 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:31,198 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:31,204 - INFO - Layer: 9\n",
      "2025-05-11 12:32:31,205 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:31,205 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:31,206 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:31,348 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:31,349 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:31,392 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:31,393 - INFO - Layer: 10\n",
      "2025-05-11 12:32:31,393 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:31,394 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:31,394 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:31,560 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:31,561 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:31,600 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:31,601 - INFO - Layer: 11\n",
      "2025-05-11 12:32:31,601 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:31,601 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:31,602 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:31,799 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:31,800 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:31,860 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:31,861 - INFO - Layer: 12\n",
      "2025-05-11 12:32:31,861 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:31,862 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:31,862 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:31,996 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:31,997 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:32,035 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:32,036 - INFO - Layer: 13\n",
      "2025-05-11 12:32:32,036 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:32,037 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:32,037 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:32,195 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:32,196 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:32,236 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:32,237 - INFO - Layer: 14\n",
      "2025-05-11 12:32:32,238 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:32,238 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:32,239 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:32,503 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:32,504 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:32,558 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:32,559 - INFO - Layer: 15\n",
      "2025-05-11 12:32:32,560 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:32,560 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:32,560 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:32,730 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:32,731 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:32,765 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:32,766 - INFO - Layer: 16\n",
      "2025-05-11 12:32:32,767 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:32,767 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:32,767 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:32,970 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:32,971 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:33,043 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:33,044 - INFO - Layer: 17\n",
      "2025-05-11 12:32:33,045 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:33,045 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:33,045 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:33,240 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:33,241 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:33,273 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:33,274 - INFO - Layer: 18\n",
      "2025-05-11 12:32:33,275 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:33,275 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:33,275 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:33,472 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:33,473 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:33,518 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:33,519 - INFO - Layer: 19\n",
      "2025-05-11 12:32:33,519 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:33,519 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:33,520 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:33,722 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:33,723 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:33,762 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:33,763 - INFO - Layer: 20\n",
      "2025-05-11 12:32:33,763 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:33,764 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:33,764 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:33,904 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:33,906 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:33,966 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:33,968 - INFO - Layer: 21\n",
      "2025-05-11 12:32:33,968 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:33,969 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:33,970 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:34,187 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:34,188 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:34,222 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:34,223 - INFO - Layer: 22\n",
      "2025-05-11 12:32:34,224 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:34,224 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:34,225 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:34,451 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:34,452 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:34,493 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:34,495 - INFO - Layer: 23\n",
      "2025-05-11 12:32:34,495 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:34,498 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:34,499 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:34,687 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:34,688 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:34,729 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:34,731 - INFO - Layer: 24\n",
      "2025-05-11 12:32:34,731 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:34,732 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:34,732 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:34,920 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:34,921 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:34,965 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:34,966 - INFO - Layer: 25\n",
      "2025-05-11 12:32:34,966 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:34,966 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:34,967 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:35,136 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:35,137 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:35,186 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:35,187 - INFO - Layer: 26\n",
      "2025-05-11 12:32:35,187 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:35,188 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:35,188 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:35,360 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:35,361 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:35,405 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:35,406 - INFO - Layer: 27\n",
      "2025-05-11 12:32:35,406 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:35,407 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:35,407 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:35,561 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:35,562 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:35,600 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:35,601 - INFO - Layer: 28\n",
      "2025-05-11 12:32:35,601 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:35,602 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:35,603 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:35,774 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:35,775 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:35,816 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:35,817 - INFO - Layer: 29\n",
      "2025-05-11 12:32:35,817 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:35,818 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:35,818 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:35,973 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:35,974 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:36,011 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:36,012 - INFO - Layer: 30\n",
      "2025-05-11 12:32:36,012 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:36,016 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:36,018 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:36,192 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:36,193 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:36,245 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:36,246 - INFO - Layer: 31\n",
      "2025-05-11 12:32:36,247 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:36,247 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:36,247 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:36,441 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:36,442 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:36,487 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:36,488 - INFO - Layer: 32\n",
      "2025-05-11 12:32:36,488 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:36,488 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:36,489 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:36,671 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:36,672 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:36,711 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:36,712 - INFO - Layer: 33\n",
      "2025-05-11 12:32:36,714 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:36,715 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:36,715 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:36,885 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:36,886 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:36,932 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:36,933 - INFO - Layer: 34\n",
      "2025-05-11 12:32:36,934 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:36,934 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:36,934 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:37,085 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:37,087 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:37,131 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:37,132 - INFO - Layer: 35\n",
      "2025-05-11 12:32:37,132 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:37,133 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:37,133 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:37,314 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:37,314 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:37,349 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:37,350 - INFO - Layer: 36\n",
      "2025-05-11 12:32:37,350 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:37,351 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:37,351 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:37,554 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:37,555 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:37,597 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:37,598 - INFO - Layer: 37\n",
      "2025-05-11 12:32:37,598 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:37,599 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:37,599 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:37,783 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:37,784 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:37,817 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:37,818 - INFO - Layer: 38\n",
      "2025-05-11 12:32:37,818 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:37,818 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:37,819 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:37,992 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:37,992 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:38,022 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:38,023 - INFO - Layer: 39\n",
      "2025-05-11 12:32:38,023 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:38,023 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:38,024 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:38,248 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:38,249 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:38,310 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:38,311 - INFO - Layer: 40\n",
      "2025-05-11 12:32:38,311 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:38,312 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:38,312 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:38,473 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:38,474 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:38,514 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:38,515 - INFO - Layer: 41\n",
      "2025-05-11 12:32:38,515 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:38,516 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:38,516 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:38,719 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:38,720 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:38,759 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:38,760 - INFO - Layer: 42\n",
      "2025-05-11 12:32:38,760 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:38,760 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:38,761 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:38,952 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:38,953 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:39,012 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:39,013 - INFO - Layer: 43\n",
      "2025-05-11 12:32:39,014 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:39,014 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:39,014 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:39,176 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:39,177 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:39,222 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:39,223 - INFO - Layer: 44\n",
      "2025-05-11 12:32:39,223 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:39,224 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:39,224 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:39,401 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:39,403 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:39,441 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:39,442 - INFO - Layer: 45\n",
      "2025-05-11 12:32:39,442 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:39,445 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:39,446 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:39,574 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:39,575 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:39,611 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:39,612 - INFO - Layer: 46\n",
      "2025-05-11 12:32:39,612 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:39,613 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:39,613 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:39,778 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:39,779 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:39,815 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:39,816 - INFO - Layer: 47\n",
      "2025-05-11 12:32:39,816 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:39,818 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:39,818 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:39,972 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:39,972 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:40,006 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 12:32:40,007 - INFO - Layer: 48\n",
      "2025-05-11 12:32:40,007 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 12:32:40,007 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 12:32:40,008 - INFO - loading ff weights and bias\n",
      "2025-05-11 12:32:40,204 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 12:32:40,205 - INFO - loading attention weights and bias\n",
      "2025-05-11 12:32:40,242 - INFO - Using LoRA weights for multi head layers!!\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8832634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is AI?\\n\\nAI is a term that refers to artificial intelligence, which is a branch of computer science that deals with the design and implementation of intelligent systems.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"What is AI?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
