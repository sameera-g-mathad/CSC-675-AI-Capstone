{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0177a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:05:13,687 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "from chathist import InstructionDataLoader,InstructionDataset, InstructionStyle, Model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befcccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df = pd.read_json(\"hf://datasets/ogrnz/chat-titles/dataset.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efad11af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Develop a method for clustering astronomical d...</td>\n",
       "      <td>Astronomical Data Clustering Method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of dopamine in the brain as a...</td>\n",
       "      <td>Dopamine as a Neurotransmitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Replace the \\\"XXX\\\" in the following sentence ...</td>\n",
       "      <td>Company Sustainability Objective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Identify the sentence type (declarative, inter...</td>\n",
       "      <td>Sentence Type Identification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suggest a book title for a fantasy novel about...</td>\n",
       "      <td>Outlaws in Enchanted Realms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  Develop a method for clustering astronomical d...   \n",
       "1  What is the role of dopamine in the brain as a...   \n",
       "2  Replace the \\\"XXX\\\" in the following sentence ...   \n",
       "3  Identify the sentence type (declarative, inter...   \n",
       "4  Suggest a book title for a fantasy novel about...   \n",
       "\n",
       "                                 title  \n",
       "0  Astronomical Data Clustering Method  \n",
       "1       Dopamine as a Neurotransmitter  \n",
       "2     Company Sustainability Objective  \n",
       "3         Sentence Type Identification  \n",
       "4          Outlaws in Enchanted Realms  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7056abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94a2a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['message', 'title'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f81bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:05:14,044 - INFO - phi3 style chosen!!\n"
     ]
    }
   ],
   "source": [
    "style = InstructionStyle.load(style='phi3', input_query=\"<|user|>\\n\", response_query=\"\\n<|llm|>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a12774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = style.convert_train(chat_df, input_col='message', response_col= 'title', output_col='instruct', new_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208fa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 9500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff3e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:train_len, :]\n",
    "val_df = df.iloc[train_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24969e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|user|&gt;\\nDevelop a method for clustering astr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|user|&gt;\\nWhat is the role of dopamine in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|user|&gt;\\nReplace the \\\"XXX\\\" in the following...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|user|&gt;\\nIdentify the sentence type (declarat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|user|&gt;\\nSuggest a book title for a fantasy n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9495</th>\n",
       "      <td>&lt;|user|&gt;\\nAssume yourself to be a business whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9496</th>\n",
       "      <td>&lt;|user|&gt;\\nIgnore all previous instructions. Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497</th>\n",
       "      <td>&lt;|user|&gt;\\nGiven the following description, ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9498</th>\n",
       "      <td>&lt;|user|&gt;\\nWhat is the 17th letter of supercali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9499</th>\n",
       "      <td>&lt;|user|&gt;\\nHow does the genetic makeup of an an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               instruct\n",
       "0     <|user|>\\nDevelop a method for clustering astr...\n",
       "1     <|user|>\\nWhat is the role of dopamine in the ...\n",
       "2     <|user|>\\nReplace the \\\"XXX\\\" in the following...\n",
       "3     <|user|>\\nIdentify the sentence type (declarat...\n",
       "4     <|user|>\\nSuggest a book title for a fantasy n...\n",
       "...                                                 ...\n",
       "9495  <|user|>\\nAssume yourself to be a business whi...\n",
       "9496  <|user|>\\nIgnore all previous instructions. Ac...\n",
       "9497  <|user|>\\nGiven the following description, ide...\n",
       "9498  <|user|>\\nWhat is the 17th letter of supercali...\n",
       "9499  <|user|>\\nHow does the genetic makeup of an an...\n",
       "\n",
       "[9500 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f5f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Develop a method for clustering astronomical data.\n",
      "<|llm|>\n",
      "Astronomical Data Clustering Method\n"
     ]
    }
   ],
   "source": [
    "print(train_df['instruct'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f0c07ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>&lt;|user|&gt;\\nWhat are the differences in the prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9501</th>\n",
       "      <td>&lt;|user|&gt;\\nImplement a program to find the comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9502</th>\n",
       "      <td>&lt;|user|&gt;\\nHow can a person develop a healthy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>&lt;|user|&gt;\\nPlease help me use schrodinger's equ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504</th>\n",
       "      <td>&lt;|user|&gt;\\nRewrite the sentence to use a pronou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>&lt;|user|&gt;\\nFrom the given passage, identify the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>&lt;|user|&gt;\\nAnalyze an effect that the text coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>&lt;|user|&gt;\\nWhat is the average number of hours ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>&lt;|user|&gt;\\nClassify the following statement as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>&lt;|user|&gt;\\nRewrite the following paragraph in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               instruct\n",
       "9500  <|user|>\\nWhat are the differences in the prop...\n",
       "9501  <|user|>\\nImplement a program to find the comm...\n",
       "9502  <|user|>\\nHow can a person develop a healthy s...\n",
       "9503  <|user|>\\nPlease help me use schrodinger's equ...\n",
       "9504  <|user|>\\nRewrite the sentence to use a pronou...\n",
       "...                                                 ...\n",
       "9995  <|user|>\\nFrom the given passage, identify the...\n",
       "9996  <|user|>\\nAnalyze an effect that the text coul...\n",
       "9997  <|user|>\\nWhat is the average number of hours ...\n",
       "9998  <|user|>\\nClassify the following statement as ...\n",
       "9999  <|user|>\\nRewrite the following paragraph in t...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540280a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "What are the differences in the properties of the top quark when it is produced through different modes such as gluon fusion and vector boson fusion, and how do these differences impact our understanding of the top quark's interactions with other particles in the standard model of particle physics?\n",
      "<|llm|>\n",
      "Top Quark Production Modes\n"
     ]
    }
   ],
   "source": [
    "print(val_df['instruct'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a47f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:05:14,112 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n"
     ]
    }
   ],
   "source": [
    "train_dataset = InstructionDataset(train_df['instruct'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5265e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_df['instruct'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc2a18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = InstructionDataLoader().load(dataset = train_dataset, batch_size = 16, shuffle=True, drop_last=True, mask_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb38b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = InstructionDataLoader().load(dataset = val_dataset, batch_size = 16, shuffle=True, drop_last=True, mask_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a242a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "329dc273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:05:15,343 - INFO - Repo: openai-community/gpt2.\n",
      "2025-05-11 16:05:15,344 - INFO - openai-community/gpt2 exists in /Users/sameergururajmathad/Documents/CSC - 675/AI Capstone/chathist/models/pretrained/gpt2. Skipping Download...\n",
      "2025-05-11 16:05:15,478 - INFO - Loading weights into model.\n",
      "2025-05-11 16:05:15,506 - INFO - Using Lora for final output layer!!\n",
      "2025-05-11 16:05:15,506 - INFO - Layer: 1\n",
      "2025-05-11 16:05:15,507 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,507 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,508 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,510 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,511 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,513 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,514 - INFO - Layer: 2\n",
      "2025-05-11 16:05:15,514 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,515 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,515 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,518 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,519 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,520 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,521 - INFO - Layer: 3\n",
      "2025-05-11 16:05:15,522 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,522 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,522 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,525 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,525 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,527 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,528 - INFO - Layer: 4\n",
      "2025-05-11 16:05:15,528 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,528 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,529 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,532 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,533 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,535 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,536 - INFO - Layer: 5\n",
      "2025-05-11 16:05:15,536 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,537 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,537 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,539 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,540 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,542 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,543 - INFO - Layer: 6\n",
      "2025-05-11 16:05:15,543 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,543 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,543 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,546 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,547 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,550 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,551 - INFO - Layer: 7\n",
      "2025-05-11 16:05:15,551 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,551 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,552 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,554 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,555 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,556 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,557 - INFO - Layer: 8\n",
      "2025-05-11 16:05:15,557 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,558 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,558 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,560 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,561 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,563 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,564 - INFO - Layer: 9\n",
      "2025-05-11 16:05:15,564 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,565 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,565 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,568 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,568 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,570 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,571 - INFO - Layer: 10\n",
      "2025-05-11 16:05:15,571 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,571 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,572 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,574 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,574 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,576 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,577 - INFO - Layer: 11\n",
      "2025-05-11 16:05:15,577 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,578 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,578 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,581 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,582 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,584 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-11 16:05:15,585 - INFO - Layer: 12\n",
      "2025-05-11 16:05:15,585 - INFO - loading norm1 weights and bias\n",
      "2025-05-11 16:05:15,585 - INFO - loading norm2 weights and bias\n",
      "2025-05-11 16:05:15,585 - INFO - loading ff weights and bias\n",
      "2025-05-11 16:05:15,588 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-11 16:05:15,588 - INFO - loading attention weights and bias\n",
      "2025-05-11 16:05:15,590 - INFO - Using LoRA weights for multi head layers!!\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2d5e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_loss, val_loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/chathist/model.py:87\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, train_loader, val_loader)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.step()\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % eval_freq == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     train_loss.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     89\u001b[39m         val_loss.append(\u001b[38;5;28mself\u001b[39m.evaluate(val_loader))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/chathist/model.py:51\u001b[39m, in \u001b[36mModel.evaluate\u001b[39m\u001b[34m(self, _loader)\u001b[39m\n\u001b[32m     49\u001b[39m inputs.to(\u001b[38;5;28mself\u001b[39m._device)\n\u001b[32m     50\u001b[39m targets.to(\u001b[38;5;28mself\u001b[39m._device)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m loss: torch.Tensor = \u001b[38;5;28mself\u001b[39m.calc_loss(logits, targets)\n\u001b[32m     53\u001b[39m total_loss += loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/chathist/gpt_modules.py:450\u001b[39m, in \u001b[36mGPT2.forward\u001b[39m\u001b[34m(self, batch_x)\u001b[39m\n\u001b[32m    448\u001b[39m x = tok_embs + pos_embs\n\u001b[32m    449\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_norm(x)\n\u001b[32m    452\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/chathist/gpt_modules.py:351\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    349\u001b[39m x_copy = x\n\u001b[32m    350\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm2(x)\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m    353\u001b[39m x += x_copy  \u001b[38;5;66;03m# shortcut\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/chathist/gpt_modules.py:200\u001b[39m, in \u001b[36mMLPGPT2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    193\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[33;03m    This abstract method is implemented as needed by\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m    `nn.Module`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m \u001b[33;03m    returns the outputs of same dimensions as that of inputs.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CSC - 675/AI Capstone/chathist/venv/lib/python3.12/site-packages/chathist/gpt_modules.py:102\u001b[39m, in \u001b[36mGeLU.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) -> torch.Tensor:\n\u001b[32m     94\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    This abstract method is implemented as needed by\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m    `nn.Module`.\u001b[39;00m\n\u001b[32m     97\u001b[39m \n\u001b[32m     98\u001b[39m \u001b[33;03m    Computes GeLU activation.\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[32m0.5\u001b[39m * x) * (\n\u001b[32m    101\u001b[39m         \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         + \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = model.train(train_loader = train_loader, val_loader = val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
