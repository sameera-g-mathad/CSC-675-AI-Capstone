{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b5c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chathist import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06564b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 15:40:56,758 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-14 15:40:57,850 - INFO - phi3 style chosen!!\n",
      "2025-05-14 15:40:58,181 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,441 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:58,490 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,526 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:58,577 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,612 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:58,665 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,699 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:58,753 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,789 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:58,846 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,881 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:58,938 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:58,982 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,043 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,077 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,135 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,169 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,225 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,260 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,317 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,352 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,409 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,445 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,502 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,536 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,593 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,627 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,684 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,719 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,776 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,810 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,867 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,903 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:40:59,960 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:40:59,994 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,051 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:41:00,086 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,143 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:41:00,178 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,236 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:41:00,270 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,327 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:41:00,362 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,419 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:41:00,454 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,511 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 15:41:00,546 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 15:41:00,831 - INFO - Using Lora for final output layer!!\n",
      "2025-05-14 15:41:00,843 - INFO - Trained model exists on saved path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-medium_chat_title_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff490b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 15:41:02,170 - INFO - Formatting prompt into phi3 style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NLP Overview'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_model.generate(\"What is NLP? I don't think I know it? Can you explain it to me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10e6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
