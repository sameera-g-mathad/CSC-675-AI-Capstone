{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024a1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chathist import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538e3177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:57:50,603 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-14 14:57:51,607 - INFO - alpaca style chosen!!\n",
      "2025-05-14 14:57:52,183 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:52,511 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:52,627 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:52,708 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:52,819 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:52,891 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:53,002 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:53,074 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:53,184 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:53,256 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:53,365 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:53,438 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:53,549 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:53,622 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:53,733 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:53,806 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:53,917 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:53,989 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:54,099 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:54,172 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:54,285 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:54,357 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:54,470 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:54,542 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:54,655 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:54,727 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:54,840 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:54,910 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:55,021 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:55,101 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:55,213 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:55,287 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:55,401 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:55,475 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:55,589 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:55,662 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:55,774 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:55,846 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:55,956 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:56,030 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:56,142 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:56,213 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:56,325 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:56,399 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:56,513 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:56,584 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:56,697 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:56,769 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:56,883 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:56,955 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:57,069 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:57,140 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:57,253 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:57,327 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:57,440 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:57,511 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:57,624 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:57,697 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:57,810 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:57,884 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:57,997 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:58,071 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:58,183 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:58,256 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:58,368 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:58,441 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:58,553 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:58,627 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:58,740 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:58,812 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:58,925 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:58,998 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:59,111 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:59,183 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:59,296 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:59,368 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:59,482 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:59,555 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:59,668 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:59,742 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:57:59,855 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:57:59,927 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:00,039 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:00,114 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:00,227 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:00,301 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:00,414 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:00,486 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:00,599 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:00,670 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:00,783 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:00,856 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:00,968 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:01,041 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:01,154 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-14 14:58:01,227 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-14 14:58:01,650 - INFO - Using Lora for final output layer!!\n",
      "2025-05-14 14:58:01,657 - INFO - Trained model exists on saved path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-xl_indian_history_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f217c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:58:07,400 - INFO - Formatting prompt into alpaca style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Gangetic plains saw active cultural developments, the rise of territorial states, and the birth of religions like Jainism and Buddhism.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_model.generate(\"\"\"Describe the impact of the Gangetic plains on cultural and historical developments in ancient India.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccbcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
