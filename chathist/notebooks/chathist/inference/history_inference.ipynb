{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024a1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chathist\n",
    "from chathist import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f58a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 10:02:35,021 - WARNING - train is not present in the config file, falling back to defaults!!!\n"
     ]
    }
   ],
   "source": [
    "chathist.config.load_config(config_path='conf/train', config_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538e3177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 10:02:35,028 - INFO - Setting tokenizer using tiktoken with encoding: gpt2\n",
      "2025-05-16 10:02:36,005 - INFO - alpaca style chosen!!\n",
      "2025-05-16 10:02:36,562 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:36,864 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:36,978 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:37,057 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:37,170 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:37,254 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:37,367 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:37,444 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:37,561 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:37,634 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:37,745 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:37,818 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:37,929 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:38,002 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:38,113 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:38,187 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:38,298 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:38,370 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:38,481 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:38,555 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:38,666 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:38,739 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:38,853 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:38,942 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:39,054 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:39,126 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:39,239 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:39,312 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:39,428 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:39,501 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:39,614 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:39,687 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:39,806 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:39,880 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:39,994 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:40,067 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:40,181 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:40,253 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:40,366 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:40,438 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:40,551 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:40,623 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:40,735 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:40,807 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:40,919 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:40,991 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:41,104 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:41,174 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:41,285 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:41,355 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:41,466 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:41,537 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:41,648 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:41,718 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:41,828 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:41,898 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:42,008 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:42,079 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:42,189 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:42,262 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:42,381 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:42,453 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:42,565 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:42,639 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:42,751 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:42,822 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:42,933 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:43,004 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:43,115 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:43,185 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:43,295 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:43,368 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:43,481 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:43,551 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:43,662 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:43,735 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:43,848 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:43,920 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:44,033 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:44,103 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:44,213 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:44,286 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:44,402 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:44,479 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:44,596 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:44,671 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:44,783 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:44,855 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:44,968 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:45,042 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:45,155 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:45,228 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:45,348 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:45,425 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:45,539 - INFO - Using LoRA weights for ff layers!!\n",
      "2025-05-16 10:02:45,612 - INFO - Using LoRA weights for multi head layers!!\n",
      "2025-05-16 10:02:46,035 - INFO - Using Lora for final output layer!!\n",
      "2025-05-16 10:02:46,047 - INFO - Trained model exists on saved path /home/smathad/ai_capstone/chathist/models/finetuned/gpt2-xl_indian_history_finetuned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f217c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 10:03:01,934 - INFO - Formatting prompt into alpaca style\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kalidasa, the greatest poet of gupta period, wrote Meghadutam.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_model.generate(\"\"\"who was the greatest poet of gupta period.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccbcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
